Monday Plan:
For the final of the first three weeks, I will try to find an optimum architecture for the SVM-like model. To do this, various hyperparameters will be varied until a seemingly optimal combination is found. Among these hyperparameters are learning rate, optimizer (stochastic gradient descent, rmsprop, momentum, and others), support vector size, input image dimension, and training batch size.
To find a good combination of hyperparameters, I will do a gridsearch - first try coarsely distributed combinations, and then break down the areas around the best results into finer grids, continuing until results do not show improvement.
From the classification accuracy resulting after the first few epochs of training, each combination will be compared and the top few will be selected for further training. These will be compared by validation performance after extensive training, and the best will be evaluated on the training dataset and its performance logged to be compared with the coming weeks' models.

Weekly Report:
I created a gridsearch program to test different values of hyperparameter space.
For this experiment, the constants were the number of data points used to train each model, the image generator's random seed for image shuffling, and the range and type of random image transformations used to supplement the data (including small shifts, shears, rescalings, and rotations of the images).
The variables were learning rates, architecture sizes, data batch sizes, image dimensions, activation functions, momentums, losses, and optimizers. The exact values that were tested can be seen on lines 41-48 of the file 'svm_gridsearch.py'.
Unfortunately, predicting the runtime of the program at approximately half of a year led me to greatly reduce the possible values and amount of data to train on until the runtime approached about two days. I overestimated the time that each datapoint would take by approximately a factor of two, so the comparison finished in just under a day. Saving all 624 model's weights was not practical due to limited storage, especially setting a standard for the next 15 weeks. Instead, a string describing the model's hyperparameters, associated with a dictionary of its training and validation accuracy and loss, was added to a list for each model. The entire list was saved to a pickle file at the end of the program, which means the list can be loaded from the file at a later time.
This list was used to compare the performance of each model based on this short period of initial training. Because different loss functions produce loss values that are not directly comprable, the accuracies of the models were compared. Because the validation accuracy suggests how the model will generalize, it is a good measure for performance. However, watching some training progress during the second week shows that validation performance can fluctuate significantly. Thus, I have selected the top three models for further training by order of the product of validation and training accuracy. The top model has training accuracy of 7.32% and validation accuracy of 12.4%.
The final results of training the models will be updated when I push the coming week's plan; I still need to do the training.
Update 1:
Each of the top three hyperparameter settings were trained using 100 cycles over the training data. Their final performances are as follows:
Model 1:  Training accuracy 9.90%, validation accuracy 9.73%,  test accuracy 8.85%
Model 2: Training accuracy 10.4%,  validation accuracy 8.85%, test accuracy 11.9%
Model 3:  Training accuracy 9.33%, validation accuracy 9.73%,  test accuracy 9.29%
