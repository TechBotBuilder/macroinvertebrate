Monday Plan:
For the final of the first three weeks, I will try to find an optimum architecture for the SVM-like model. To do this, various hyperparameters will be varied until a seemingly optimal combination is found. Among these hyperparameters are learning rate, optimizer (stochastic gradient descent, rmsprop, momentum, and others), support vector size, input image dimension, and training batch size.
To find a good combination of hyperparameters, I will do a gridsearch - first try coarsely distributed combinations, and then break down the areas around the best results into finer grids, continuing until results do not show improvement.
From the classification accuracy resulting after the first few epochs of training, each combination will be compared and the top few will be selected for further training. These will be compared by validation performance after extensive training, and the best will be evaluated on the training dataset and its performance logged to be compared with the coming weeks' models.

