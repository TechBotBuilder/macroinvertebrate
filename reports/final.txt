Abstract:
This is a comparison of different artificial neural network architectures' performances on a small dataset of images of 21 classes of macroinvertebrates. The best model will be used in an application to identify macroinvertebrates in the field. This also serves as practical experience with neural networks for me.
Four different network architectures were used: SVM-like, where the inputs map to a layer of nonlinear units through a static weight matrix, which then is mapped to class probabilities (a softmax layer) through a learnable weight matrix; simple feedforward, where the input is mapped to class probabilities by progressing through several nonlinear hidden layers and learnable weight matrices; convolutional, where each layer is expanded into several feature maps by small, shared weight matrices (learnable kernals), and then the next layer is produced with pooling; and recurrent/highway, where the input feeds into a layer that feeds into itself for a predetermined number of steps until it feeds to an output, creating a deep network with only three weight matrices.
Brief results:
The convolutional architecture performed best of the four.

Notes:
I noticed some interesting results in gridsearch.
Looking at highway gridsearch, the distribution of optimizers along the loss axis seems more or less random - eg, sorting the 151 gridsearched results by training loss from least loss to greatest,
among the 75 models with least loss, 25 are SGD, 25 are adam, and 25 are rmsprop, while
among the 76 models with greatest loss, 24 are SGD, 27 are adam, and 25 are rmsprop.
The lack of clustering probably demonstrates the similarity of the optimizers before they have had time to 'build up momentum' (or each optimizer's equivalent) in the 3 epochs each model was trained for, or the randomness associated with not seeding the model's weight initializers.
Similar analyses on different variables for the highway model revealed a correlation between:
 - deeper networks and lower loss (Depth: # in lower half of loss / # in upper half of loss)
   - 20: 18 / 11
   - 13: 19 / 11
   -  8: 13 / 16
   -  5: 13 / 18
   -  3: 12 / 19
 - For hidden sizes, smaller has lower loss than bigger: (Lower half of loss/upper half of loss)
   - 64  : 28 / 2
   - 512 : 19 / 10
   - 1024: 14 / 16
   - 2048: 11 / 21
   - 5096:  3 / 26
 - Moderately strong outperformance of sigmoid over relu:
   - sigmoid: 53 / 24
   -    relu: 22 / 51
 - Optimizers about equal performance (as above):
   -     SGD: 25 / 24
   -    Adam: 25 / 26
   - RMSprop: 25 / 25

Similarly, in the convolutional gridsearch:
 - strongly show relu outperformed sigmoid
   - sigmoid:  3 / 32
   -    relu: 33 /  3
 - A slight, possibly nonsignificant, preference for 16x16 image over 32x32
   - 16x16: 19 / 16
   - 32x32: 17 / 19
 - dropout follows the usual pattern of initially reducing performance:
   -  0.0: 15 /  8
   -  0.5: 12 / 12
   - 0.75:  9 / 15
 - no difference for kernal size:
   - 3: 18 / 18
   - 5: 18 / 17
 - L2 loss also initially reduces performance:
   -    0: 14 /  9
   - 1e-5: 13 / 11
   - 1e-3:  9 / 15

Dense feedforward gridsearch:
 There were two loss functions tested, mean and categorical crossentropy, which cannot be compared directly, so are listed separately here:
Mean squared error loss:
 - Optimizers - Adamax over SGD over RMSprop:
   -     SGD: 115 / 75
   -  Adamax:  32 / 16
   - RMSprop:  22 / 78
 - Hidden sizes - 2048 just over 256 over 32:
   -   32: 34 / 78
   -  256: 45 / 67
   - 2048: 46 / 68
 - Batch sizes - 64 over 256:
   -  64: 90 / 78
   - 256: 79 / 91
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 87 / 79
   - 32x32: 82 / 90
 - Activations - sigmoid over relu:
   - sigmoid: 132 /  36
   -    relu:  37 / 133
 - Net depths - 4 layers over 2 layers:
   - 2: 74 / 94
   - 4: 95 / 75
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 44
   - None: 44 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 51 / 43
   - 2^-6: 44 / 52
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 16 / 34
   - 2^-6: 34 / 16
Categorical crossentropy loss:
 - Optimizers - SGD over Adamax over RMSprop:
   -     SGD: 114 / 74
   -  Adamax:  22 / 26
   - RMSprop:  31 / 67
 - Hidden sizes - 32 over 256 over 2048:
   -   32: 89 / 23
   -  256: 60 / 52
   - 2048: 18 / 92
 - Batch sizes - 64 over 256:
   -  64: 94 / 74
   - 256: 73 / 93
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 85 / 79
   - 32x32: 82 / 88
 - Activations:
   - sigmoid: 112 /  52
   -    relu:  55 / 115
 - Net depths - 4 layers over 2 layers:
   - 2: 80 / 88
   - 4: 87 / 79
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 43
   - None: 43 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 53 / 41
   - 2^-6: 41 / 53
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 18 / 31
   - 2^-6: 31 / 18

Support-vector-machine(-like) models, which also gridsearched over categorical crossentropy and mean squared error. (I wrote 'svm_analysis.py' to demonstrate how I am getting this data; the others I analyzed in a Python REPL session.)
Mean squared error loss:
 - Optimizers - Adamax over SGD way over RMSprop:
   -     SGD: 136 / 80
   -  Adamax:  16 /  8
   - RMSprop:   4 / 68
 - Hidden sizes - 4096 over 1024 over 256:
   -  256: 44 / 60
   - 1024: 53 / 51
   - 4096: 59 / 45
 - Batch sizes - No preference:
   -  64: 78 / 78
   - 256: 78 / 78
 - Image sizes - 16x16 over 32x32:
   - 16x16: 92 / 64
   - 32x32: 64 / 92
 - Activations - sigmoid very much over relu:
   - sigmoid: 124 /  32
   -    relu:  32 / 124
 - Momentum (SGD only) - No preference:
   -  0.9: 36 / 36
   -  0.5: 36 / 36
   - None: 36 / 36
 - Learning rate (SGD only) - No preference:
   - 2^-1: 36 / 36
   - 2^-4: 36 / 36
   - 2^-8: 36 / 36
 - Learning rate (RMSprop only) - 1/256 over 1/2 over 1/16:
   - 2^-1: 10 / 14
   - 2^-4:  9 / 15
   - 2^-8: 17 /  7
Categorical crossentropy loss:
 - Optimizers - Adamax very much prefered over SGD over RMSprop:
   -     SGD: 117 / 99
   -  Adamax:  24 / 0
   - RMSprop:  15 / 57
 - Hidden sizes - 256 over 1024 over 4096:
   -  256: 70 / 34
   - 1024: 51 / 53
   - 4096: 35 / 69
 - Batch sizes - 256 over 64:
   -  64: 74 / 82
   - 256: 82 / 74
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 80 / 76
   - 32x32: 76 / 80
 - Activations - sigmoid over relu:
   - sigmoid: 94 / 62
   -    relu: 62 / 94
 - Momentum (SGD only) - None over 0.5 over 0.9:
   -  0.9: 32 / 40
   -  0.5: 37 / 35
   - None: 39 / 33
 - Learning rate (SGD only) - 1/256 strongly over 1/16 over 1/2:
   - 2^-1: 10 / 62
   - 2^-4: 28 / 44
   - 2^-8: 70 /  2
 - Learning rate (RMSprop only) - 1/256 strongly over 1/16 same as 1/2:
   - 2^-1:  8 / 16
   - 2^-4:  8 / 16
   - 2^-8: 20 /  4

The above data mostly follows the expected patterns - techniques like dropout and weight regularization that are meant to reduce overfitting reduce initial performance, and the best learning rates, activations, and optimizers vary, while smaller minibatches are mostly preferred for batch size.
However, it seems that smaller image sizes are slightly preferred, which does not intuitively make sense. Also, it seems that cross-entropy loss models do better with smaller hidden sizes, which does not make much sense either, and yet mean-squared-error loss models indeed do better with bigger hidden sizes. It is probable that larger models and image sizes have better performance after more thorough training, and that this is a result of the second problem below.

Issues:
There were several problems with the investigation.
The lack of seeding the random number generators for weight initialization was a fault, since the performance of a certain hyperparameter combination could not be exactly reproduced.
Nor were the comparisons among hyperparameter combinations after only three epochs of training very 'fair,' since little can be told about the eventual accuracy of a model with such a small amount of training due to 'plateus' of little improvement leading up to large gains and the effect of noisiness that minibatch updates have on model convergence. With the hardware available, however, a more lengthy gridsearch would have been infeasible.
Another problem was that, after the first few convolutional models were tried, the code refused to save any more training histories - data which helps in hand-tuning hyperparameters, a process described in Stanford's convnet course notes (http://cs231n.github.io/neural-networks-3/#baby). As a result of this, sensible hyperparameter patches to do gridsearch over were rather blindly chosen.
These problems might be fixed in the following ways:
The first, trivially, by adding seeding code to the gridsearch and training program.
The last two, by a Bayesian method, essentially using machine learning to predict which hyperparameter combination might work best and which areas of hyperparameter space should be explored ti get more data. However, nothing would change the fact that more epochs of training than the three performed are needed to get very informative results from each model.

Result Summary:
What follows are the best models (by test accuracy) from each architecture, each trained for at least 100 epochs over the data.
SVM:  Training 10.4%,  validation  8.85%, test 11.9% (100 epochs)
FF:   Training 11.3%,  validation 10.6%,  test 13.7% (100 epochs)
      Training 98.5%,  validation 24.8%,  test 19.9% (3000 epochs)
Conv: Training 80.5%,  validation 20.4%,  test 23.0% (100 epochs)
      Training 95.5%,  validation 23.0%,  test 28.3% (1000 epochs)
RNN:  Training  7.23%, validation  7.08%, test  7.08% (100 epochs)
      Training 16.1%,  validation  9.73%, test 16.4% (1500 epochs)
In short, the best models trained for the convolutional architecture and the deep feedforward architecture achieved performances of approximately 28% and 20% test accuracy respectively, and both experienced severe overfitting.
Further training with careful experimentation and tuning of parameters could improve the performance of the SVM-like or highway-like architectures, but the time investment would be inhibitive.
The next steps will be to tune the convolutional and feedforward architectures and to train them for a long period (up to a week), starting with testing a convolutional model with depth greater than two convolutional layers and using spatial dropout to reduce overfitting. The results of the thorough analysis of gridsearch listed above should be of help in finetuning hyperparameters.
Once it is trained, the best model can then be used in a mobile phone application to give suggestions to novices about the species of a macroinvertebrate, although they might be only 25% helpful. It might be that adding a feature to the app to match the images taken with a similar-looking example picture and uploading the 32x32 grayscale images to a single source would drive the creation of a much larger dataset, which would discourage overfitting and thus increase test accuracy. Another feature of the app should be to download new network weights as they become available.
Other aspects to try are not using a difference-of-gaussian filter and using colored inputs instead of grayscale.

