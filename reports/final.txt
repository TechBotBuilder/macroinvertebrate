I noticed some interesting results in gridsearch.
Looking at highway gridsearch, the distribution of optimizers along the loss axis seems more or less random - eg, sorting the 151 gridsearched results by training loss from least loss to greatest,
among the 75 models with least loss, 25 are SGD, 25 are adam, and 25 are rmsprop, while
among the 76 models with greatest loss, 24 are SGD, 27 are adam, and 25 are rmsprop.
The lack of clustering probably demonstrates the similarity of the optimizers before they have had time to 'build up momentum' (or each optimizer's equivalent) in the 3 epochs each model was trained for, or the randomness associated with not seeding the model's weight initializers.
Similar analyses on different variables for the highway model revealed a correlation between:
 - deeper networks and lower loss (Depth: # in lower half of loss / # in upper half of loss)
   - 20: 18 / 11
   - 13: 19 / 11
   -  8: 13 / 16
   -  5: 13 / 18
   -  3: 12 / 19
 - For hidden sizes, smaller has lower loss than bigger: (Lower half of loss/upper half of loss)
   - 64  : 28 / 2
   - 512 : 19 / 10
   - 1024: 14 / 16
   - 2048: 11 / 21
   - 5096:  3 / 26
 - Moderately strong outperformance of sigmoid over relu:
   - sigmoid: 53 / 24
   -    relu: 22 / 51
 - Optimizers about equal performance (one more time):
   -     sgd: 25 / 24
   -    adam: 25 / 26
   - rmsprop: 25 / 25

Similarly, in the convolutional gridsearch:
 - strongly show relu outperformed sigmoid
   - sigmoid:  3 / 32
   -    relu: 33 /  3
 - A slight, possibly nonsignificant, preference for 16x16 image over 32x32
   - 16x16: 19 / 16
   - 32x32: 17 / 19
 - dropout follows the usual pattern of initially reducing performance:
   -  0.0: 15 /  8
   -  0.5: 12 / 12
   - 0.75:  9 / 15
 - no difference from kernal size:
   - 3: 18 / 18
   - 5: 18 / 17
 - L2 loss also initially reduces performance:
   -    0: 14 /  9
   - 1e-5: 13 / 11
   - 1e-3:  9 / 15

Following are similar inspections of the other model architectures.

Dense feedforward gridsearch:
 There were two loss functions tested, mean and categorical crossentropy, which cannot be compared directly, so are listed separately here:
Mean squared error loss:
 - Optimizers - Adamax over SGD over RMSprop:
   -     SGD: 115 / 75
   -  Adamax:  32 / 16
   - RMSprop:  22 / 78
 - Hidden sizes - 2048 just over 256 over 32:
   -   32: 34 / 78
   -  256: 45 / 67
   - 2048: 46 / 68
 - Batch sizes - 64 over 256:
   -  64: 90 / 78
   - 256: 79 / 91
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 87 / 79
   - 32x32: 82 / 90
 - Activations - sigmoid over relu:
   - sigmoid: 132 /  36
   -    relu:  37 / 133
 - Net depths - 4 layers over 2 layers:
   - 2: 74 / 94
   - 4: 95 / 75
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 44
   - None: 44 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 51 / 43
   - 2^-6: 44 / 52
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 16 / 34
   - 2^-6: 34 / 16
Categorical crossentropy loss:
 - Optimizers - SGD over Adamax over RMSprop:
   -     SGD: 114 / 74
   -  Adamax:  22 / 26
   - RMSprop:  31 / 67
 - Hidden sizes:
   -   32: 89 / 23
   -  256: 60 / 52
   - 2048: 18 / 92
 - Batch sizes - 64 over 256:
   -  64: 94 / 74
   - 256: 73 / 93
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 85 / 79
   - 32x32: 82 / 88
 - Activations:
   - sigmoid: 112 /  52
   -    relu:  55 / 115
 - Net depths - 4 layers over 2 layers:
   - 2: 80 / 88
   - 4: 87 / 79
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 43
   - None: 43 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 53 / 41
   - 2^-6: 41 / 53
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 18 / 31
   - 2^-6: 31 / 18

Support-vector-machine(-like) models, which also gridsearched over categorical crossentropy and mean squared error. (I wrote 'svm_analysis.py' to demonstrate how I am getting this data; the others I analyzed in a Python REPL session.)
Mean squared error loss:
 - Optimizers - Adamax over SGD way over RMSprop:
   -     SGD: 136 / 80
   -  Adamax:  16 /  8
   - RMSprop:   4 / 68
 - Hidden sizes - 4096 over 1024 over 256:
   -  256: 44 / 60
   - 1024: 53 / 51
   - 4096: 59 / 45
 - Batch sizes - No preference:
   -  64: 78 / 78
   - 256: 78 / 78
 - Image sizes - 16x16 over 32x32:
   - 16x16: 92 / 64
   - 32x32: 64 / 92
 - Activations - sigmoid very much over relu:
   - sigmoid: 124 /  32
   -    relu:  32 / 124
 - Momentum (SGD only) - No preference:
   -  0.9: 36 / 36
   -  0.5: 36 / 36
   - None: 36 / 36
 - Learning rate (SGD only) - No preference:
   - 2^-1: 36 / 36
   - 2^-4: 36 / 36
   - 2^-8: 36 / 36
 - Learning rate (RMSprop only) - 1/256 over 1/2 over 1/16:
   - 2^-1: 10 / 14
   - 2^-4:  9 / 15
   - 2^-8: 17 /  7
Categorical crossentropy loss:
 - Optimizers - Adamax very much prefered over SGD over RMSprop:
   -     SGD: 117 / 99
   -  Adamax:  24 / 0
   - RMSprop:  15 / 57
 - Hidden sizes - 256 over 1024 over 4096:
   -  256: 70 / 34
   - 1024: 51 / 53
   - 4096: 35 / 69
 - Batch sizes - 256 over 64:
   -  64: 74 / 82
   - 256: 82 / 74
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 80 / 76
   - 32x32: 76 / 80
 - Activations - sigmoid over relu:
   - sigmoid: 94 / 62
   -    relu: 62 / 94
 - Momentum (SGD only) - None over 0.5 over 0.9:
   -  0.9: 32 / 40
   -  0.5: 37 / 35
   - None: 39 / 33
 - Learning rate (SGD only) - 1/256 strongly over 1/16 over 1/2:
   - 2^-1: 10 / 62
   - 2^-4: 28 / 44
   - 2^-8: 70 /  2
 - Learning rate (RMSprop only) - 1/256 strongly over 1/16 same as 1/2:
   - 2^-1:  8 / 16
   - 2^-4:  8 / 16
   - 2^-8: 20 /  4


There were several problems with the investigation.
The lack of seeding the random number generators for weight initialization was a fault, since the performance of a certain hyperparameter combination could not be exactly reproduced.
Nor were the comparisons among hyperparameter combinations after only three epochs of training very 'fair,' since little can be told about the eventual accuracy of a model with such a small amount of training due to 'plateus' of little improvement leading up to large gains and the effect of noisiness that minibatch updates have on model convergence. With the hardware available, however, a more lengthy gridsearch would have been infeasible.
Another problem was that, after the first few convolutional models were tried, the code refused to save any more training histories - data which helps in hand-tuning hyperparameters, a process described in Stanford's convnet course notes (http://cs231n.github.io/neural-networks-3/#baby). As a result of this, sensible hyperparameter patches to do gridsearch over were rather blindly chosen.
These problems might be fixed in the following ways:
The first, trivially, by adding seeding code to the gridsearch and training program.
The last two, by a bayesian method, essentially using machine learning to predict which hyperparameter combination might work best and which areas of hyperparameter space should be explored.

Result Summary:
What follows are the best models (by test accuracy) from each architecture, each trained for at least 100 epochs over the data.
SVM:  Training 10.4%,  validation  8.85%, test 11.9% (100 epochs)
FF:   Training 11.3%,  validation 10.6%,  test 13.7% (100 epochs)
      Training 98.5%,  validation 24.8%,  test 19.9% (3000 epochs)
Conv: Training 80.5%,  validation 20.4%,  test 23.0% (100 epochs)
      Training 95.5%,  validation 23.0%,  test 28.3% (1000 epochs)
RNN:  Training  7.23%, validation  7.08%, test  7.08% (100 epochs)
      Training 16.1%,  validation  9.73%, test 16.4% (1500 epochs)

In short, the best models trained for the convolutional architecture and the deep feedforward architecture both achieved similar performance of approximately 25% test accuracy, and both experienced severe overfitting. The convolutional network could have been improved with increased depth.
Further training with careful experimentation and tuning of parameters could also improve the performance of the SVM-like or highway-like architectures, but the time investment would be inhibitive.
The next steps will be to tune the convolutional and feedforward architectures and to train them for a long period (possibly a week), starting with testing a convolutional model with depth greater than two. The results of the thorough analysis of gridsearch listed above should be of help in finetuning hyperparameters. Once it is trained, the best model can then be used in a mobile phone application to give suggestions to novices about the species of a macroinvertebrate.
Another aspect to try might be using raw images instead of those with a difference-of-gaussian filter, or using colored inputs instead of grayscale.
As mentioned in weekly reports, using spatial dropout in the convolutional models may reduce overfitting.
