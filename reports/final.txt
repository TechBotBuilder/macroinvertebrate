I noticed some interesting results in gridsearch.
Looking at highway gridsearch, the distribution of optimizers along the loss axis seems more or less random - eg, sorting the 151 gridsearched results by training loss from least loss to greatest,
among the 75 models with least loss, 25 are SGD, 25 are adam, and 25 are rmsprop, while
among the 76 models with greatest loss, 24 are SGD, 27 are adam, and 25 are rmsprop.
The lack of clustering probably demonstrates the similarity of the optimizers before they have had time to 'build up momentum' (or each optimizer's equivalent) in the 3 epochs each model was trained for, or the randomness associated with not seeding the model's weight initializers.
Similar cluster analyses on different variables for the highway model revealed a correlation between:
 - deeper networks and lower loss (Depth: # in lower half of loss / # in upper half of loss)
   - 20: 18 / 11
   - 13: 19 / 11
   -  8: 13 / 16
   -  5: 13 / 18
   -  3: 12 / 19
 - For hidden sizes, smaller has lower loss than bigger: (Lower half of loss/upper half of loss)
   - 64  : 28 / 2
   - 512 : 19 / 10
   - 1024: 14 / 16
   - 2048: 11 / 21
   - 5096:  3 / 26
 - Moderately strong outperformance of sigmoid over relu:
   - sigmoid: 53 / 24
   -    relu: 22 / 51
 - Optimizers about equal performance (one more time):
   -     sgd: 25 / 24
   -    adam: 25 / 26
   - rmsprop: 25 / 25

Similarly, in the convolutional gridsearch:
 - strongly show relu outperformed sigmoid
   - sigmoid:  3 / 32
   -    relu: 33 /  3
 - A slight, possibly nonsignificant, preference for 16x16 image over 32x32
   - 16x16: 19 / 16
   - 32x32: 17 / 19
 - dropout follows the usual pattern of initially reducing performance:
   -  0.0: 15 /  8
   -  0.5: 12 / 12
   - 0.75:  9 / 15
 - no difference from kernal size:
   - 3: 18 / 18
   - 5: 18 / 17
 - L2 loss also initially reduces performance:
   -    0: 14 /  9
   - 1e-5: 13 / 11
   - 1e-3:  9 / 15

We're on a roll, so why not inspect the other model architectures too?

Dense feedforward gridsearch:
 There were two loss functions tested, mean and categorical crossentropy, which cannot be compared directly, so are listed separately here:
Mean squared error loss:
 - Optimizers - Adamax over SGD over RMSprop:
   -     SGD: 115 / 75
   -  Adamax:  32 / 16
   - RMSprop:  22 / 78
 - Hidden sizes - 2048 just over 256 over 32:
   -   32: 34 / 78
   -  256: 45 / 67
   - 2048: 46 / 68
 - Batch sizes - 64 over 256:
   -  64: 90 / 78
   - 256: 79 / 91
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 87 / 79
   - 32x32: 82 / 90
 - Activations - sigmoid over relu:
   - sigmoid: 132 /  36
   -    relu:  37 / 133
 - Net depths - 4 layers over 2 layers:
   - 2: 74 / 94
   - 4: 95 / 75
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 44
   - None: 44 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 51 / 43
   - 2^-6: 44 / 52
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 16 / 34
   - 2^-6: 34 / 16
Categorical crossentropy loss:
 - Optimizers - SGD over Adamax over RMSprop:
   -     SGD: 114 / 74
   -  Adamax:  22 / 26
   - RMSprop:  31 / 67
 - Hidden sizes:
   -   32: 89 / 23
   -  256: 60 / 52
   - 2048: 18 / 92
 - Batch sizes - 64 over 256:
   -  64: 94 / 74
   - 256: 73 / 93
 - Image sizes - 16x16 just over 32x32:
   - 16x16: 85 / 79
   - 32x32: 82 / 88
 - Activations:
   - sigmoid: 112 /  52
   -    relu:  55 / 115
 - Net depths - 4 layers over 2 layers:
   - 2: 80 / 88
   - 4: 87 / 79
 - Momentum (SGD only) - 0.9 over none:
   -  0.9: 51 / 43
   - None: 43 / 51
 - Learning rate (SGD only) - 1/8 over 1/64:
   - 2^-3: 53 / 41
   - 2^-6: 41 / 53
 - Learning rate (RMSprop only) - 1/64 over 1/8:
   - 2^-3: 18 / 31
   - 2^-6: 31 / 18

Support-vector-machine(-like) models, which also gridsearched over categorical crossentropy and mean squared error:


There were several problems with the investigation.
The lack of seeding the random number generators for weight initialization was a fault, since the performance of a certain hyperparameter combination could not be exactly reproduced.
Nor were the comparisons among hyperparameter combinations after only three epochs of training very 'fair,' since little can be told about the eventual accuracy of a model from its first (approximately) 100 updates due to 'plateus' of little improvement leading up to large gains and the effect of noisiness that minibatch updates have on model convergence. With the hardware available, however, a more lengthy gridsearch would have been infeasible.
Another problem was that, after the first few convolutional models were tried, the code refused to save any more training histories - data which helps in hand-tuning hyperparameters, a process described in Stanford's convnet course notes (http://cs231n.github.io/neural-networks-3/#baby). As a result of this, sensible hyperparameter patches to do gridsearch over were rather blindly chosen. For example, after gridsearching for the highway architecture,  

In short, the best models trained for the convolutional architecture and the deep feedforward architecture both achieved similar performance of approximately 25% test accuracy, and both experienced severe overfitting. The convolutional network could have been improved with increased depth.
Further training with careful experimentation and tuning of parameters could also improve the performance of the SVM-like or highway-like architectures, but the time investment would be inhibitive.
The next steps will be to tune the convolutional and feedforward architectures and to train them for a long period (possibly a week), starting with testing a convolutional model with depth greater than two. The results of the thorough analysis of gridsearch listed above should be of help in finetuning hyperparameters. The best model can then be used in a mobile phone application to give suggestions to novices about what species a macroinvertebrate is.
