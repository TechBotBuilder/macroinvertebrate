Week 6&7 Report:
The problem is probably with image parsing, since inspection of the values returned from parseBitmap reveals all zero values. I narrowed the problem to grayscale conversion, and finally found that I was writing integers to a bitmap and then trying to directly read the pixel values back from the bitmap, instead of going back and forth storing the value in a rgb channel. After verifying that this problem was fixed, the app still classified everything as definitely a midge. I altered the final layer output to linear instead of softmax, revealing that the zero probabilities the app assigned to non-midge classes were really only very close to zero, but were not indicative of an error in the layer operations code. I inspected the bias values for the final layer that the network read, thinking there might have been an error in the layer matrix data-loading code that read a large value for that index, but all of the biases were in the same range. Since I was uncertain about the form in which Keras' dependency on the python imaging library (PIL) impacted the input data, I had assumed that I should divide the input image data by 255 to keep it in the range between 0 and 1, but when I removed the division, the predictions showed nonzero values across multiple classes and were not dominated by midges.
I will have to verify that this is the correct input range by further interrogating PIL documentation or inspecting the actual values keras outputs from its image preprocessing class.
